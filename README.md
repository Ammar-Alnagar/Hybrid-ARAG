                      
project is Finished 
---

Aya Expanse 32B: Leading the Pack

Model Rankings and Insights

1. Aya Expanse 32B (Ranked 1st)

Aya Expanse dominates the leaderboard with superior performance across all key metrics:

Highest CoT EM and F1 Scores: Achieves an exceptional 83.0 EM and 87.0 F1, showcasing unparalleled reasoning capabilities with precise accuracy and partial match recognition.

Best Retrieval Hit Ratio: At 93.2%, it retrieves the most relevant context, ensuring seamless alignment between reasoning and retrieved knowledge.

Lowest Latency: With a blazing-fast 1.7 seconds, Aya Expanse excels in real-time applications, setting the benchmark for speed.

Top Contextual Chunking Performance: Scoring 84.6 EM and 88.5 F1, it efficiently handles complex contexts, delivering precise and relevant answers.

Outstanding Retrieval Precision and Recall: With 92.0% precision and 93.4% recall, it ensures highly relevant and comprehensive retrievals.

Best Query Efficiency Ratio (QER): At 0.8, Aya Expanse optimizes query rewriting and retrieval, making it highly efficient and adaptable.



---

2. ChatGPT-4.0 (Ranked 2nd)

ChatGPT-4.0 closely follows, delivering robust performance:

Strong CoT EM and F1 Scores: Scores 82.1 EM and 86.4 F1, excelling in reasoning tasks but slightly trailing Aya Expanse.

High Retrieval Hit Ratio: Achieves 91.2%, showcasing reliable retrieval capabilities.

Efficient Latency: Processes queries in 1.9 seconds, maintaining competitive speed.

Impressive Contextual Chunking: Scores 83.7 EM and 87.5 F1, demonstrating strong comprehension and contextual understanding.

Good Retrieval Precision and Recall: Precision at 88.9% and recall at 90.1%, proving its competence in retrieving relevant content.

QER: At 0.9, ChatGPT-4.0â€™s query efficiency is slightly less optimal compared to Aya Expanse.



---

3. Mamba2 Model (Ranked 3rd)

The Mamba2 Model delivers solid performance but trails behind the leaders:

CoT EM and F1: Scores 80.5 EM and 84.8 F1, showcasing good reasoning but lagging behind the top models.

Strong Retrieval Hit Ratio: At 89.6%, it retrieves relevant contexts effectively, though not as efficiently as Aya Expanse.

Latency: Achieves a reasonable 2.0 seconds, slightly slower than Aya Expanse and ChatGPT-4.0.

Contextual Chunking Performance: Scores 82.4 EM and 86.2 F1, showing proficiency in understanding contextual information.

Balanced Precision and Recall: Precision of 86.7% and recall of 88.5% reflect solid retrieval capabilities.

QER: At 1.0, Mamba2 is efficient but not as optimized as the top two models.



---

4. Claude Sonnet 3.5 (Ranked 4th)

Claude Sonnet 3.5 is a reliable model but ranks below the competition:

CoT Performance: Scores 79.3 EM and 83.2 F1, showing adequate reasoning but falling short of leaders.

Good Retrieval Hit Ratio: At 88.9%, it performs well in retrieving context but lacks the precision of Aya Expanse and ChatGPT-4.0.

Latency: Processes queries in 2.1 seconds, slightly slower than the other models.

Contextual Chunking: Scores 81.0 EM and 85.5 F1, indicating effective but less optimal context handling.

Retrieval Precision and Recall: Precision at 85.1% and recall at 87.2%, showing adequate retrieval capabilities.

QER: At 1.1, Claude Sonnet 3.5 has room for improvement in query rewriting and retrieval optimization.



---

5. Llama 3 70B (Ranked 5th)

Llama 3 70B ranks last due to its relatively lower performance:

CoT Performance: Scores 78.4 EM and 82.1 F1, lagging behind in reasoning and match quality.

Retrieval Hit Ratio: At 87.5%, it shows the lowest alignment with external knowledge.

Latency: Takes 2.3 seconds, slower than other models in the ranking.

Contextual Chunking: Scores 80.2 EM and 84.3 F1, performing adequately but below expectations.

Precision and Recall: Precision at 84.6% and recall at 85.9%, the lowest among the group.

QER: At 1.2, its query efficiency indicates room for optimization.



---

Hybrid-ARAG: Techniques and Progress

The Hybrid-ARAG framework integrates advanced reasoning and retrieval techniques for superior performance.

Implemented Features:

1. Query Rewriting (Completed)


2. Contextual Chunking (Completed)


3. Chain-of-Thought (CoT) (Completed)


4. Agentic Retrieval-Augmented Generation (RAG) (Completed)


5. Graph-based RAG (Completed)


6. Reinforced CoT (RCOT) (Completed)


7. Multi-Agent Structure (Completed)


8. Integration with Mamba2 Models (Completed)


9. Reinforcement Learning Integration (Work in Progress)




---

Models Used So Far:

1. Llama 3.1 70B


2. Falcon Mamba 2 & Codestral Mamba


3. Aya Expanse 32B (Best performer)


4. ChatGPT-4.0 (Baseline/Judge)


5. Llama 3.3 70B


6. Qwen2.5 QWQ


7. Meraj Mini Model (Experimental use)
