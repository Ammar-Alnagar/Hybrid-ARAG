Model Name	CoT EM	CoT F1	CoT Retrieval Hit Ratio	CoT Latency (s)	Contextual Chunking EM	Contextual Chunking F1	Contextual Chunking Hit Ratio	Contextual Chunking Latency (s)	Retrieval Precision	Retrieval Recall	Query Efficiency Ratio	Combined Score
Llama 3 70B	78.4	82.1	87.5	2.3	80.2	84.3	89.1	2.7	84.6	85.9	1.2	83.6
Aya Expanse 32B	83	87	93.2	1.7	84.6	88.5	94	2	92	93.4	0.8	88.38333333
Claude Sonnet 3.5	79.3	83.2	88.9	2.1	81	85.5	90.6	2.6	85.1	87.2	1.1	84.75
Mamba2 Model	80.5	84.8	89.6	2	82.4	86.2	91.4	2.5	86.7	88.5	1	85.81666667
ChatGPT-4.0	82.1	86.4	91.2	1.9	83.7	87.5	92.8	2.3	88.9	90.1	0.9	87.28333333






________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________
## Hybrid-ARAG



1-Implement Query Re-wrtiing  (Done)

2-Implement Contextual Chunking (Wip)

3-Implement COT (Done)

4-Implement Agentic Graph if Possible (Wip)

5-Integrate the usage of mamba2 models (Done)

________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

## used models so far :

1-llama 3.1 70B

2-falcon mamba 2 & codestral mamba 

3-Aya Expanse 32B (best so far)

4-Gpt 4o (Judge/Baseline)

5- llama 3.3 70b

6-Qwen2.5 qwq
________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

